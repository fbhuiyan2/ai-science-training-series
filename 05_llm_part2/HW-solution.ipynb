{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8f03fe-35e4-4d31-9738-3d6e7350c6b4",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Load in a generative model using the HuggingFace pipeline. Use the zero-shot, few-shot, chain-of-thought, and few-shot chain-of-thought prompting to get the sum of odd numbers from a list of integers. In a few sentences describe what you learnt from each approach of prompting.\n",
    "- Next, play around with the temperature parameter. In a few sentences describe what you changes you notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9677404-47a8-4cef-a51d-f04f6c0c51c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: 40230.0 / 40960.0 MB\n",
      "GPU 1: 15001.0 / 40960.0 MB\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "\n",
    "GPUs = GPUtil.getGPUs()\n",
    "for gpu in GPUs:\n",
    "    print(f\"GPU {gpu.id}: {gpu.memoryUsed} / {gpu.memoryTotal} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b1d92ec-da0d-4b1a-9800-eb0b28089740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68049155-88a4-41d1-985c-9607facc44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in dir():\n",
    "    var = eval(var_name)\n",
    "    if torch.is_tensor(var) and var.is_cuda:\n",
    "        del var\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8705401e-5561-4e49-b87f-13b93cd6bd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.cuda' from '/soft/applications/conda/2024-08-08/mconda3/lib/python3.11/site-packages/torch/cuda/__init__.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640ef8c9-0b46-4127-890a-a1c7e0e4f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ebfd40-de8a-4784-97b1-e9c312b28126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 26 18:28:17 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:07:00.0 Off |                    0 |\n",
      "| N/A   22C    P0             56W /  400W |   40230MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          Off |   00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   21C    P0             54W /  400W |       3MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   3652255      C   ...conda/2024-08-08/mconda3/bin/python      31936MiB |\n",
      "|    0   N/A  N/A   4044088      C   ...conda/2024-08-08/mconda3/bin/python       8280MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4273cf02-68d6-4f7e-a30a-4d3b4328c61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter huggingfacehub api token:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = getpass('Enter huggingfacehub api token: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa9ac487-f0ce-418b-b505-bef6c1baf8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f8a54b3c024660aaa2148963b5085b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `device` and `device_map` are specified. `device` will override `device_map`. You will most likely encounter unexpected behavior. Please remove `device` and keep `device_map`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# load model\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\" #\"tiiuae/falcon-7b-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir='../')\n",
    "# model = \"tiiuae/falcon-40b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pipeline = transformers.pipeline(\"text-generation\",\n",
    "                                        model=model,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        torch_dtype=torch.bfloat16,\n",
    "                                        trust_remote_code=True,\n",
    "                                        device_map=\"auto\",\n",
    "                                        device=device\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2034337b-e123-4a85-a887-fc25a6f922c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load remote model\n",
    "from langchain.llms import HuggingFaceHub\n",
    "# model = \"tiiuae/falcon-7b-instruct\"\n",
    "# model = \"tiiuae/falcon-40b-instruct\"\n",
    "remote_model = HuggingFaceHub(\n",
    "    repo_id=model_name,\n",
    "    model_kwargs={\"temperature\": 0.5,\n",
    "                  \"max_new_tokens\": 2056},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2a2ad46d-5ff3-4b28-8ecd-aba40d414758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(input, host=\"remote\", **kwargs):\n",
    "    prompt = f\"#### User: \\n{input}\\n\\n#### Response from model:\"\n",
    "    response = \"\"\n",
    "    # print(prompt)\n",
    "    if host.lower() == \"local\":\n",
    "      print(\"invoking llm at Google Colab\")\n",
    "      if 'max_length' not in kwargs:\n",
    "        kwargs['max_length'] = 1000\n",
    "\n",
    "      \n",
    "        _response = pipeline(prompt,\n",
    "                          #max_length=500,\n",
    "                          do_sample=True,\n",
    "                          top_k=10,\n",
    "                          num_return_sequences=1,\n",
    "                          eos_token_id=tokenizer.eos_token_id,\n",
    "                          **kwargs,\n",
    "                          )\n",
    "      response = _response[0]['generated_text']\n",
    "\n",
    "    elif host.lower() == \"remote\":\n",
    "      print(\"invoking llm at Huggingface Hub\")\n",
    "      if \"max_length\" in kwargs:\n",
    "        kwargs['max_new_tokens'] = kwargs['max_length']\n",
    "\n",
    "      response = remote_model.invoke(prompt, **kwargs)\n",
    "\n",
    "    else:\n",
    "      print (\"invalid host value, must be 'remote' or 'local'\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d83c345-eb27-411c-896c-44b3acc0b6f9",
   "metadata": {},
   "source": [
    "# Zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9bc60df3-3e68-47e4-b4bc-92a69bcbf5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "\n",
      "Problem:\n",
      "Q: The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
      "A:\n",
      "\n",
      "\n",
      "#### Response from model:\n",
      "\n",
      "Let's identify the odd numbers in the given group:\n",
      "\n",
      "15, 5, 13, 7, 1\n",
      "\n",
      "Now, let's add them up:\n",
      "\n",
      "15 + 5 + 13 + 7 + 1 = 41\n",
      "\n",
      "The sum of the odd numbers is 41, which is an odd number, not an even number.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt =  \"\"\"\n",
    "Problem:\n",
    "Q: The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
    "A:\n",
    "\"\"\"\n",
    "#15 + 5 + 13 + 7 + 1 = 41\n",
    "zero_shot_prompt_explanation = get_completion(zero_shot_prompt)\n",
    "print(zero_shot_prompt_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b17acc7-1fa6-4896-8352-c837875b469c",
   "metadata": {},
   "source": [
    "The model was right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2440e3f5-a894-49d6-9689-89cd509d12a0",
   "metadata": {},
   "source": [
    "# Few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2ec7d2c6-24d8-4d26-b7d7-19926926c440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "I will provide you with a few examples of a math problem.\n",
      "Follow the pattern to solve a problem.\n",
      "Example1:\n",
      "Q: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
      "A: False\n",
      "\n",
      "Example2:\n",
      "Q: The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.\n",
      "A: True.\n",
      "\n",
      "Now solve the following problem. Explain the steps involved.\n",
      "Q: The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
      "A:\n",
      "\n",
      "\n",
      "#### Response from model:\n",
      "\n",
      "To determine if the odd numbers in the group add up to an even number, we first need to identify all the odd numbers in the group. Then, we add them together and check if the sum is even.\n",
      "\n",
      "Step 1: Identify the odd numbers in the group.\n",
      "Odd numbers: 15, 5, 13, 7, 1\n",
      "\n",
      "Step 2: Add the odd numbers together.\n",
      "Sum = 15 + 5 + 13 + 7 + 1 = 41\n",
      "\n",
      "Step 3: Check if the sum is even.\n",
      "Since 41 is not divisible by 2, it is an odd number.\n",
      "\n",
      "Conclusion: The odd numbers in the group do not add up to an even number. So, the answer is False.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt =  \"\"\"I will provide you with a few examples of a math problem.\n",
    "Follow the pattern to solve a problem.\n",
    "Example1:\n",
    "Q: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: False\n",
    "\n",
    "Example2:\n",
    "Q: The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.\n",
    "A: True.\n",
    "\n",
    "Now solve the following problem. Explain the steps involved.\n",
    "Q: The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
    "A:\n",
    "\"\"\"\n",
    "#15 + 5 + 13 + 7 + 1 = 41\n",
    "few_shot_prompt_explanation = get_completion(few_shot_prompt)\n",
    "print(few_shot_prompt_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a203e085-2514-4645-81b1-2b1cade119e8",
   "metadata": {},
   "source": [
    "Again right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d505d09-1a3c-4e4f-a22e-9c17e2a9355f",
   "metadata": {},
   "source": [
    "# Few-shot COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "841145f3-b76f-41b8-9b2d-987991a7c270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "You are a mathematical genius.\n",
      "The template of the question is given as Q: [question]. The template of the answer is given as A: [answer].\n",
      "Help me solve the problem below using the examples given.\n",
      "\n",
      "Examples:\n",
      "Q: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
      "A: The given numbers are: (4, 8, 9, 15, 12, 2, 1). Next select the odd numbers (9, 15, 1).  Adding all the selected odd numbers (9, 15, 1) gives 25. 25 is an odd number, hence the answer is False.\n",
      "\n",
      "Q: The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.\n",
      "A: The given numbers are: (17, 10, 19, 4, 8, 12, 24). Next select the odd numbers (17, 19).  Adding all the selected odd numbers (17, 19) gives 36. 36 is an even number, hence the answer is True.\n",
      "\n",
      "Q: The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.\n",
      "A: The given numbers are: (16, 11, 14, 4, 8, 13, 24). Next select the odd numbers (11, 13).  Adding all the selected odd numbers (11, 13) gives 24. 24 is an even number, hence the answer is True.\n",
      "\n",
      "Q: The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.\n",
      "A: The given numbers are: (17, 9, 10, 12, 13, 4, 2). Next select the odd numbers (17, 9, 13).  Adding all the selected odd numbers (17, 9, 13) gives 39. 39 is an odd number, hence the answer is False.\n",
      "\n",
      "Problem:\n",
      "Q: The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
      "A:\n",
      "\n",
      "\n",
      "#### Response from model: \n",
      "The given numbers are: (15, 32, 5, 13, 82, 7, 1). Next select the odd numbers (15, 5, 13, 7, 1).  Adding all the selected odd numbers (15, 5, 13, 7, 1) gives 41. 41 is an odd number, hence the answer is False. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_COT_prompt =  \"\"\"You are a mathematical genius.\n",
    "The template of the question is given as Q: [question]. The template of the answer is given as A: [answer].\n",
    "Help me solve the problem below using the examples given.\n",
    "\n",
    "Examples:\n",
    "Q: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The given numbers are: (4, 8, 9, 15, 12, 2, 1). Next select the odd numbers (9, 15, 1).  Adding all the selected odd numbers (9, 15, 1) gives 25. 25 is an odd number, hence the answer is False.\n",
    "\n",
    "Q: The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.\n",
    "A: The given numbers are: (17, 10, 19, 4, 8, 12, 24). Next select the odd numbers (17, 19).  Adding all the selected odd numbers (17, 19) gives 36. 36 is an even number, hence the answer is True.\n",
    "\n",
    "Q: The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.\n",
    "A: The given numbers are: (16, 11, 14, 4, 8, 13, 24). Next select the odd numbers (11, 13).  Adding all the selected odd numbers (11, 13) gives 24. 24 is an even number, hence the answer is True.\n",
    "\n",
    "Q: The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.\n",
    "A: The given numbers are: (17, 9, 10, 12, 13, 4, 2). Next select the odd numbers (17, 9, 13).  Adding all the selected odd numbers (17, 9, 13) gives 39. 39 is an odd number, hence the answer is False.\n",
    "\n",
    "Problem:\n",
    "Q: The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
    "A:\n",
    "\"\"\"\n",
    "#15 + 5 + 13 + 7 + 1 = 41\n",
    "few_shot_COT_prompt_explanation = get_completion(few_shot_COT_prompt)\n",
    "print(few_shot_COT_prompt_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf88c9e-5d76-4943-8c90-03cd7189f831",
   "metadata": {},
   "source": [
    "The model is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3cc804-efaa-4f10-9742-641a47e7265f",
   "metadata": {},
   "source": [
    "# Few-shot prompting (varying temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "002a6ad8-ad2a-4142-9250-431e70d4dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "I will provide you with a few examples of a math problem.\n",
      "Follow the pattern to solve a problem.\n",
      "Example1:\n",
      "Q: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
      "A: False\n",
      "\n",
      "Example2:\n",
      "Q: The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.\n",
      "A: True.\n",
      "\n",
      "Now solve the following problem. Explain the steps involved.\n",
      "Q: The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
      "A:\n",
      "\n",
      "\n",
      "#### Response from model: False\n",
      "To solve this problem, we need to find the odd numbers in the given group and then add them together. The odd numbers in the group are 15, 5, 13, and 7. Adding these numbers together gives us:\n",
      "\n",
      "15 + 5 + 13 + 7 = 40\n",
      "\n",
      "Since 40 is an even number, the statement is True.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt =  \"\"\"I will provide you with a few examples of a math problem.\n",
    "Follow the pattern to solve a problem.\n",
    "Example1:\n",
    "Q: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: False\n",
    "\n",
    "Example2:\n",
    "Q: The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.\n",
    "A: True.\n",
    "\n",
    "Now solve the following problem. Explain the steps involved.\n",
    "Q: The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "remote_model = HuggingFaceHub(\n",
    "    repo_id=model_name,\n",
    "    model_kwargs={\"temperature\": 0.9,\n",
    "                  \"max_new_tokens\": 2056},\n",
    ")\n",
    "\n",
    "def get_completion(input, host=\"remote\", **kwargs):\n",
    "    prompt = f\"#### User: \\n{input}\\n\\n#### Response from model:\"\n",
    "    response = \"\"\n",
    "    # print(prompt)\n",
    "    if host.lower() == \"local\":\n",
    "      print(\"invoking llm at Google Colab\")\n",
    "      if 'max_length' not in kwargs:\n",
    "        kwargs['max_length'] = 1000\n",
    "\n",
    "      \n",
    "        _response = pipeline(prompt,\n",
    "                          #max_length=500,\n",
    "                          do_sample=True,\n",
    "                          top_k=10,\n",
    "                          num_return_sequences=1,\n",
    "                          eos_token_id=tokenizer.eos_token_id,\n",
    "                          **kwargs,\n",
    "                          )\n",
    "      response = _response[0]['generated_text']\n",
    "\n",
    "    elif host.lower() == \"remote\":\n",
    "      print(\"invoking llm at Huggingface Hub\")\n",
    "      if \"max_length\" in kwargs:\n",
    "        kwargs['max_new_tokens'] = kwargs['max_length']\n",
    "\n",
    "      response = remote_model.invoke(prompt, **kwargs)\n",
    "\n",
    "    else:\n",
    "      print (\"invalid host value, must be 'remote' or 'local'\")\n",
    "\n",
    "    return response\n",
    "\n",
    "few_shot_prompt_explanation = get_completion(few_shot_prompt)\n",
    "print(few_shot_prompt_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "33306e4f-bb83-45c6-a9e7-165d3bbb05bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "I will provide you with a few examples of a math problem.\n",
      "Follow the pattern to solve a problem.\n",
      "Example1:\n",
      "Q: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
      "A: False\n",
      "\n",
      "Example2:\n",
      "Q: The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.\n",
      "A: True.\n",
      "\n",
      "Now solve the following problem. Explain the steps involved.\n",
      "Q: The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
      "A:\n",
      "\n",
      "\n",
      "#### Response from model:\n",
      "\n",
      "To solve this problem, we need to identify the odd numbers in the given group and then add them up to see if the sum is an even number.\n",
      "\n",
      "Step 1: Identify the odd numbers in the group.\n",
      "Odd numbers are those that have a remainder of 1 when divided by 2. In the given group, the odd numbers are: 15, 5, 13, 7, and 1.\n",
      "\n",
      "Step 2: Add the odd numbers together.\n",
      "15 + 5 + 13 + 7 + 1 = 41\n",
      "\n",
      "Step 3: Determine if the sum is an even number.\n",
      "41 is an odd number, as it has a remainder of 1 when divided by 2.\n",
      "\n",
      "Conclusion: The odd numbers in this group do not add up to an even number.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remote_model = HuggingFaceHub(\n",
    "    repo_id=model_name,\n",
    "    model_kwargs={\"temperature\": 1,\n",
    "                  \"max_new_tokens\": 2056},\n",
    ")\n",
    "\n",
    "def get_completion(input, host=\"remote\", **kwargs):\n",
    "    prompt = f\"#### User: \\n{input}\\n\\n#### Response from model:\"\n",
    "    response = \"\"\n",
    "    # print(prompt)\n",
    "    if host.lower() == \"local\":\n",
    "      print(\"invoking llm at Google Colab\")\n",
    "      if 'max_length' not in kwargs:\n",
    "        kwargs['max_length'] = 1000\n",
    "\n",
    "      \n",
    "        _response = pipeline(prompt,\n",
    "                          #max_length=500,\n",
    "                          do_sample=True,\n",
    "                          top_k=10,\n",
    "                          num_return_sequences=1,\n",
    "                          eos_token_id=tokenizer.eos_token_id,\n",
    "                          **kwargs,\n",
    "                          )\n",
    "      response = _response[0]['generated_text']\n",
    "\n",
    "    elif host.lower() == \"remote\":\n",
    "      print(\"invoking llm at Huggingface Hub\")\n",
    "      if \"max_length\" in kwargs:\n",
    "        kwargs['max_new_tokens'] = kwargs['max_length']\n",
    "\n",
    "      response = remote_model.invoke(prompt, **kwargs)\n",
    "\n",
    "    else:\n",
    "      print (\"invalid host value, must be 'remote' or 'local'\")\n",
    "\n",
    "    return response\n",
    "\n",
    "few_shot_prompt_explanation = get_completion(few_shot_prompt)\n",
    "print(few_shot_prompt_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "50c871e0-beae-437c-9bf3-41ca90d440a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoking llm at Huggingface Hub\n",
      "#### User: \n",
      "I will provide you with a few examples of a math problem.\n",
      "Follow the pattern to solve a problem.\n",
      "Example1:\n",
      "Q: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
      "A: False\n",
      "\n",
      "Example2:\n",
      "Q: The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.\n",
      "A: True.\n",
      "\n",
      "Now solve the following problem. Explain the steps involved.\n",
      "Q: The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
      "A:\n",
      "\n",
      "\n",
      "#### Response from model:\n",
      "\n",
      "To solve the problem, we need to identify the odd numbers in the group and then add them up to see if the sum is even.\n",
      "\n",
      "Step 1: Identify the odd numbers in the group.\n",
      "Odd numbers are those that have a remainder of 1 when divided by 2. In the given group, the odd numbers are: 15, 5, 13, 7, and 1.\n",
      "\n",
      "Step 2: Add up the identified odd numbers.\n",
      "15 + 5 + 13 + 7 + 1 = 41\n",
      "\n",
      "Step 3: Determine if the sum is even or odd.\n",
      "41 is an odd number because it has a remainder of 1 when divided by 2.\n",
      "\n",
      "So, the answer is False. The odd numbers in the group do not add up to an even number.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remote_model = HuggingFaceHub(\n",
    "    repo_id=model_name,\n",
    "    model_kwargs={\"temperature\": 0.3,\n",
    "                  \"max_new_tokens\": 2056},\n",
    ")\n",
    "\n",
    "def get_completion(input, host=\"remote\", **kwargs):\n",
    "    prompt = f\"#### User: \\n{input}\\n\\n#### Response from model:\"\n",
    "    response = \"\"\n",
    "    # print(prompt)\n",
    "    if host.lower() == \"local\":\n",
    "      print(\"invoking llm at Google Colab\")\n",
    "      if 'max_length' not in kwargs:\n",
    "        kwargs['max_length'] = 1000\n",
    "\n",
    "      \n",
    "        _response = pipeline(prompt,\n",
    "                          #max_length=500,\n",
    "                          do_sample=True,\n",
    "                          top_k=10,\n",
    "                          num_return_sequences=1,\n",
    "                          eos_token_id=tokenizer.eos_token_id,\n",
    "                          **kwargs,\n",
    "                          )\n",
    "      response = _response[0]['generated_text']\n",
    "\n",
    "    elif host.lower() == \"remote\":\n",
    "      print(\"invoking llm at Huggingface Hub\")\n",
    "      if \"max_length\" in kwargs:\n",
    "        kwargs['max_new_tokens'] = kwargs['max_length']\n",
    "\n",
    "      response = remote_model.invoke(prompt, **kwargs)\n",
    "\n",
    "    else:\n",
    "      print (\"invalid host value, must be 'remote' or 'local'\")\n",
    "\n",
    "    return response\n",
    "\n",
    "few_shot_prompt_explanation = get_completion(few_shot_prompt)\n",
    "print(few_shot_prompt_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf710e-0aa0-4840-acfd-5406cffd34ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

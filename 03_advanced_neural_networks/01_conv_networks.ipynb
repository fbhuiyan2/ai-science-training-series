{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "Notebook by Corey Adams, some modifications by Bethany Lusch\n",
    "\n",
    "Up until transformers, convolutions were *the* state of the art in computer vision.  In many ways and applications they still are!\n",
    "\n",
    "Large Language Models, which are what we'll focus on the rest of the series after this lecture, are really good at ordered, *tokenized data.  But there is lots of data that isn't _implicitly_ ordered like `images`, and their more general cousins `graphs`.\n",
    "\n",
    "Today's lecture focuses on computer vision models, and particularly on convolutional neural networks.  There are a ton of applications you can do with these, and not nearly enough time to get into them.  Check out the extra references file to see some publications to get you started if you want to learn more.\n",
    "\n",
    "Tip: this notebook is much faster on the GPU!\n",
    "\n",
    "\n",
    "## Convolutional Networks: A brief historical context\n",
    "\n",
    "![ImageNet Accuracy by Yearh](ImageNet.png)\n",
    "\n",
    "[reference](https://www.researchgate.net/publication/332452649_A_Roadmap_for_Foundational_Research_on_Artificial_Intelligence_in_Medical_Imaging_From_the_2018_NIHRSNAACRThe_Academy_Workshop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Building Blocks\n",
    "\n",
    "We're going to go through some examples of building blocks for convolutional networks.  To help illustate some of these, let's use an image for examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-26 06:23:41--  https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/03_advanced_neural_networks/ALCF-Staff.jpg\n",
      "Resolving proxy.alcf.anl.gov (proxy.alcf.anl.gov)... 140.221.69.69\n",
      "Connecting to proxy.alcf.anl.gov (proxy.alcf.anl.gov)|140.221.69.69|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 417835 (408K) [image/jpeg]\n",
      "Saving to: ‘ALCF-Staff.jpg.4’\n",
      "\n",
      "ALCF-Staff.jpg.4    100%[===================>] 408.04K  --.-KB/s    in 0.005s  \n",
      "\n",
      "2024-11-26 06:23:41 (76.7 MB/s) - ‘ALCF-Staff.jpg.4’ saved [417835/417835]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# wget line useful in Google Colab\n",
    "! wget https://raw.githubusercontent.com/argonne-lcf/ai-science-training-series/main/03_advanced_neural_networks/ALCF-Staff.jpg\n",
    "alcf_image = Image.open(\"ALCF-Staff.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "figure = plt.figure(figsize=(20,20))\n",
    "plt.imshow(alcf_image)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "Convolutions are a restriction of - and a specialization of - dense linear layers.  A convolution of an image produces another image, and each output pixel is a function of only it's local neighborhood of points.  This is called an _inductive bias_ and is a big reason why convolutions work for image data: neighboring pixels are correlated and you can operate on just those pixels at a time.\n",
    "\n",
    "See examples of convolutions [here](https://github.com/vdumoulin/conv_arithmetic)\n",
    "\n",
    "![image-2.png](conv_eqn.png)\n",
    "\n",
    "![image.png](conv.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply a convolution to the ALCF Staff photo:\n",
    "alcf_tensor = torchvision.transforms.ToTensor()(alcf_image)\n",
    "\n",
    "# Reshape the tensor to have a batch size of 1:\n",
    "alcf_tensor = alcf_tensor.reshape((1,) + alcf_tensor.shape)\n",
    "\n",
    "# Create a random convolution:\n",
    "# shape is: (channels_in, channels_out, kernel_x, kernel_y)\n",
    "conv_random = torch.rand((3,3,15,15))\n",
    "\n",
    "alcf_rand = torch.nn.functional.conv2d(alcf_tensor, conv_random)\n",
    "alcf_rand = (1./alcf_rand.max()) * alcf_rand\n",
    "print(alcf_rand.shape)\n",
    "alcf_rand = alcf_rand.reshape(alcf_rand.shape[1:])\n",
    "\n",
    "print(alcf_tensor.shape)\n",
    "\n",
    "rand_image = alcf_rand.permute((1,2,0)).cpu()\n",
    "\n",
    "figure = plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.imshow(rand_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "![Batch Norm](batch_norm.png)\n",
    "Reference: [Normalizations](https://arxiv.org/pdf/1903.10520.pdf)\n",
    "\n",
    "Normalization is the act of transforming the mean and moment of your data to standard values (usually 0.0 and 1.0).  It's particularly useful in machine learning since it stabilizes training, and allows higher learning rates.\n",
    "\n",
    "![Batch Normalization accelerates training](batch_norm_effect.png)\n",
    "\n",
    "Reference: [Batch Norm](https://arxiv.org/pdf/1502.03167.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply a normalization to the ALCF Staff photo:\n",
    "alcf_tensor = torchvision.transforms.ToTensor()(alcf_image)\n",
    "\n",
    "# Reshape the tensor to have a batch size of 1:\n",
    "alcf_tensor = alcf_tensor.reshape((1,) + alcf_tensor.shape)\n",
    "\n",
    "\n",
    "alcf_rand = torch.nn.functional.normalize(alcf_tensor)\n",
    "alcf_rand = alcf_rand.reshape(alcf_rand.shape[1:])\n",
    "\n",
    "print(alcf_tensor.shape)\n",
    "\n",
    "rand_image = alcf_rand.permute((1,2,0)).cpu()\n",
    "\n",
    "figure = plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.imshow(rand_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Downsampling (And upsampling)\n",
    "\n",
    "Downsampling is a critical component of convolutional and many vision models.  Because of the local-only nature of convolutional filters, learning large-range features can be too slow for convergence.  Downsampling of layers can bring information from far away closer, effectively changing what it means to be \"local\" as the input to a convolution.\n",
    "\n",
    "![Convolutional Pooling](conv_pooling.png \"Pooling\")\n",
    "\n",
    "[Reference](https://www.researchgate.net/publication/333593451_Application_of_Transfer_Learning_Using_Convolutional_Neural_Network_Method_for_Early_Detection_of_Terry's_Nail)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply a normalization to the ALCF Staff photo:\n",
    "alcf_tensor = torchvision.transforms.ToTensor()(alcf_image)\n",
    "\n",
    "# Reshape the tensor to have a batch size of 1:\n",
    "alcf_tensor = alcf_tensor.reshape((1,) + alcf_tensor.shape)\n",
    "\n",
    "\n",
    "alcf_rand = torch.nn.functional.max_pool2d(alcf_tensor, 2)\n",
    "alcf_rand = alcf_rand.reshape(alcf_rand.shape[1:])\n",
    "\n",
    "print(alcf_tensor.shape)\n",
    "\n",
    "rand_image = alcf_rand.permute((1,2,0)).cpu()\n",
    "\n",
    "figure = plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.imshow(rand_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connections\n",
    "\n",
    "One issue, quickly encountered when making convolutional networks deeper and deeper, is the \"Vanishing Gradients\" problem.  As layers were stacked on top of each other, the size of updates dimished at the earlier layers of a convolutional network.  The paper \"Deep Residual Learning for Image Recognition\" solved this by introduction \"residual connections\" as skip layers.\n",
    "\n",
    "\n",
    "Reference: [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "\n",
    "![Residual Layer](residual_layer.png)\n",
    "\n",
    "\n",
    "Compare the performance of the models before and after the introduction of these layers:\n",
    "\n",
    "![Resnet Performance vs. Plain network performance](resnet_comparison.png)\n",
    "\n",
    "If you have time to read only one paper on computer vision, make it this one!  Resnet was the first model to beat human accuracy on ImageNet and is one of the most impactful papers in AI ever published."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a ConvNet\n",
    "\n",
    "In this section we'll build and apply a conv net to the mnist dataset.  The layers here are loosely based off of the ConvNext architecture.  Why?  Because we're getting into LLM's soon, and this ConvNet uses LLM features.  ConvNext is an update to the ResNet architecture that outperforms it.\n",
    "\n",
    "[ConvNext](https://arxiv.org/abs/2201.03545)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset here is CIFAR-10 - slightly harder than MNIST but still relatively easy and computationally tractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soft/applications/conda/2024-08-08/mconda3/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import v2\n",
    "training_data = torchvision.datasets.CIFAR10(\n",
    "    # Polaris: root=\"/lus/eagle/projects/datasets/CIFAR-10/\",\n",
    "    # Polaris: download=False,\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=v2.Compose([\n",
    "        v2.ToTensor(),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.RandomResizedCrop(size=32, scale=[0.85,1.0], antialias=False),\n",
    "        v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10(\n",
    "    # Polaris: root=\"/lus/eagle/projects/datasets/CIFAR-10/\",\n",
    "    # Polaris: download=False,\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [0.8, 0.2], generator=torch.Generator().manual_seed(55))\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# The dataloader makes our dataset iterable \n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, \n",
    "    batch_size=batch_size, \n",
    "    pin_memory=True,\n",
    "    shuffle=True, \n",
    "    num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(validation_data, \n",
    "    batch_size=batch_size, \n",
    "    pin_memory=True,\n",
    "    shuffle=False, \n",
    "    num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvjklEQVR4nO3de2zd9X3/8de5+/gex4kdEydNoIVSSKpmkHq0jJIsl/2EoEQTtJUWOgSCGTTIuraZWihskxmVWtoqDX+MkVVqoGVqQKAVBqEx6pawJSVK6SUjWdoEEjvk4tuxz/X7+f3BcOcS4PNO7Hxs5/mQjhT7vPP253s55+3jc87rxJxzTgAAnGXx0AsAAJybGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCCSoRfw+6Io0uHDh1VXV6dYLBZ6OQAAI+ecBgcH1dbWpnj83R/nTLoBdPjwYbW3t4deBgDgDB06dEhz58591+snbABt2LBBX//619XT06PFixfrO9/5ji6//PL3/X91dXWSpE8tnqFkwu8RUEt9lfe6Ll6Q9a6VpJ7BYe/a85qaTb1nVPuvpWcgZ+p9ouC/7tY6//0nSRoxPjLN+vevreoztY4ZkqT6SglT72LFv3dV3JZo1VCVMtXnhiPv2kNvDJl6Z2v8z8NZszKm3pm0/7oV2c6rtMretVUx/1pJ6nPVpvrfHs97186tt23nkZP+vdsabfdB6YT/eds75H8fVChG+ofvHxi9P383EzKAfvCDH2jdunV6+OGHtXTpUj300ENauXKl9u7dq9mzZ7/n/337z27JREyphN9TVOmk/1NZVWnbnVAm5d87a+xdnfHf/eZ1O8M+ydh6q2IcQIb+WeNaYs7/Di4ft/WOT+AAsm5nVPbf55ZzVpIyacM5blx3Jm04V8wDyH+fZ2OGQSip4KzbadmHk+f4WAZQVdF4PyG979MoE/IihG984xu65ZZb9PnPf14XX3yxHn74YVVXV+uf/umfJuLHAQCmoHEfQMViUbt27dLy5ct/90PicS1fvlzbt29/R32hUNDAwMCYCwBg+hv3AXTs2DFVKhW1tLSM+X5LS4t6enreUd/V1aWGhobRCy9AAIBzQ/D3Aa1fv179/f2jl0OHDoVeEgDgLBj3FyE0NzcrkUiot7d3zPd7e3vV2tr6jvpMJqNMxvbKGgDA1Dfuj4DS6bSWLFmirVu3jn4viiJt3bpVHR0d4/3jAABT1IS8DHvdunVau3at/uAP/kCXX365HnroIeVyOX3+85+fiB8HAJiCJmQA3XDDDXrzzTd1zz33qKenRx/96Ef17LPPvuOFCQCAc1fMOcNbyc+CgYEBNTQ06OudF3u/qSpT9t+EYydOmtaTbfKf0fUJ2zwvDvrX18ywPU92rDzoXdtz1PYGwBHDO7MlaeGH/JMQEpHtdCwO+7/DvTpl286arP8+P2lMqigb3sUvSZms/zvzI9mSLfr6/c8VJ9sbOtub/ddSl7Qdn+GC/1oi491cPrJtZ7au3ru2KV009T7ZP+JdW8wb1214k2sh7n9/NVKIdMfGg+rv71d9/bvvm+CvggMAnJsYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAmJAtuPBwbyHl/Fno04h9VcTJfMq2jOp72rn1jxDbPc/3+9bUnK6be1fX+6y7b0juUqPPf35KUH/avHx7Imnqns/7xLcVSwdT72Ij/Ph8o2o59PLLVJ4aG/Xsnbed45PwiryTJJWyRUAdP+O/zatnO8YGCf7xOLF1j6l0q2W4UVYND3rXHU7ZYoLLhBpqvGKN4DNtZMcQw5Yt+tTwCAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAAQxabPgcsNllUt+8zGW9M89U8qWlVRx1f61qVZT7yHD/K+K2zLS4uWid23C+edYSZJStvJKyX8tRfnXvtXb/3hG8s88k6Qo5n98nCEnS5Jixt/94kn/zLtk0pY1loj59x6p2HLmipH/WkqR7fhUEv770DebbLR32X+fSFLk/OsLxt6K/OvLlvtCSRVDdlyl4n8sC0W/NfMICAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQxKSN4qnOJJRJ+81HZ4jkKOZtOTLZzHnetUPDNabemVTFuzZljeJx/mvJphtMvUeinKm+P3fcu7aUssUCpZz/PhwescXI1Gb8Y00a4sb4mxpbHEvJlb1rK4YYJklKpv3jWGoNtzVJqlT868uGSCDJlggVN0TlSFJk3M60YTGR8VyR8683hvwoYfgPyZR/VJLz3N88AgIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEMWmz4CT3v5f3V8znvbvW1M42rSJb2+5de6S3x9bbkDf15ptHTb2P+cd7qbW11dS7JjPDVD9kiL4qFfzzpiQpljZkx8X9c+MkKWn49Sydsv0ul0jZUrtikX8uXdG2maassYRlp0hKJfyPZ7Hkn3cnSTFDdpwhxkySVLKuxbBbksZ96Jxh8ZHtvErE/esTcf91VzyXzCMgAEAQ4z6Avva1rykWi425XHTRReP9YwAAU9yE/AnuIx/5iF544YXf/ZDkJP5LHwAgiAmZDMlk0vy8AgDg3DIhzwG99tpramtr08KFC/W5z31OBw8efNfaQqGggYGBMRcAwPQ37gNo6dKl2rRpk5599llt3LhRBw4c0Cc/+UkNDg6esr6rq0sNDQ2jl/Z2/1edAQCmrnEfQKtXr9af/umfatGiRVq5cqX+9V//VX19ffrhD394yvr169erv79/9HLo0KHxXhIAYBKa8FcHNDY26kMf+pD27dt3yuszmYwymcxELwMAMMlM+PuAhoaGtH//fs2ZM2eifxQAYAoZ9wH0hS98Qd3d3frNb36j//iP/9CnP/1pJRIJfeYznxnvHwUAmMLG/U9wr7/+uj7zmc/o+PHjmjVrlj7xiU9ox44dmjVrlqlPTJF8QyIy6Xrvvo3NC03r6Bvyj6ooFQqm3pWo5F17+Nibpt6Fgn+UyEjFFjvS2tRsqi+V/aNeYskaU+9KzD93JmWMv0lW+ecZxRK2fVjxjJl6Wzzhf1PNVFWZekfy34elctHUO5kw7POYIT9KUsLw/sJY3La/o7g1Fsg/Lidpye2R5JwhAscQq/RWb8M5boji8d3EcR9Ajz/++Hi3BABMQ2TBAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCmPCPYzhdqXRC6bTffKxvPN+7bzFuyzF7/dD/eNeW8nlT79xIn3ftiSHbJ8W6WMq79mDPYVPvfG7EVJ9J+q9l1qyZpt7plH/GYLnUZ+pdKvtn+xVkO/aVkq0+k/XPVEukbZl3zj8KTuWioViSixmyyawZaYZsMuedLPmWyGVN9emE/1qSxl/7o4phnxubR5EhpzHuv45Y3C9jjkdAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgJm0UTzyVVDzlNx/jGf/4lsO9JdM6fnvkiHdtlTGOJZn0r29uyph6lytp79rCkC1ap7/vhKm+Ou0fa3Jea5upd1O20bs2V7Kd7oWc/3ZW4raol5j8okrelvZPM1IybYi/keQMcSzxhP95JUmREt61lbLt9+F8wX+fxyq220/M2c6VWNoQU5MsmnpXIv/7iYThPJGkVMr/+ETO/77TOaJ4AACTGAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABDEpM2CK5SdFPfLqMod6/fue/BI2bSOoWKfd21doy2Dq2WGf3BTTXWtqfex4/7bebxiyyWLFW3bmfLM9JOk6qwts6su459llUjY9uGhIye9a1PJGlPvTMq2naVczn8tcf9cMkmqlPyPZ8EWd6i8IfasXLTdHVnqM8lqU+/6rK1eJf99PpQfsvWO+d9PZJwtZy5d5X/b9813e6vWL6ePR0AAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAICZtFtzwSEWVil9GVb7U4923VLDlZDXN8M/JOq/Vlu/VXO+f8RQz5DBJUrngXzt0wtRasXjaVG/Kd3O241MoDHrX5ou2nKz80Ih3bdqY1Veu2Pbhidywd23Of5dIkioVv9wuSRrO287DEUNuYDrlf3uQpKp0vX9x2T8zUJIS8SpTfSbjX1/M2e4noqjk37toy5krJwzneMI/XzLyvL/iERAAIAjzAHrppZd0zTXXqK2tTbFYTE8++eSY651zuueeezRnzhxls1ktX75cr7322nitFwAwTZgHUC6X0+LFi7Vhw4ZTXv/ggw/q29/+th5++GG9/PLLqqmp0cqVK5XPG3PcAQDTmvk5oNWrV2v16tWnvM45p4ceekhf+cpXdO2110qSvve976mlpUVPPvmkbrzxxjNbLQBg2hjX54AOHDignp4eLV++fPR7DQ0NWrp0qbZv337K/1MoFDQwMDDmAgCY/sZ1APX0vPVqtJaWljHfb2lpGb3u93V1damhoWH00t7ePp5LAgBMUsFfBbd+/Xr19/ePXg4dOhR6SQCAs2BcB1Bra6skqbe3d8z3e3t7R6/7fZlMRvX19WMuAIDpb1wH0IIFC9Ta2qqtW7eOfm9gYEAvv/yyOjo6xvNHAQCmOPOr4IaGhrRv377Rrw8cOKDdu3erqalJ8+bN01133aW/+7u/0wc/+EEtWLBAX/3qV9XW1qbrrrtuPNcNAJjizANo586d+tSnPjX69bp16yRJa9eu1aZNm/TFL35RuVxOt956q/r6+vSJT3xCzz77rKqqbNEWlXJMlZhfREh91j93Zn6zfzSIJFWirHdtTdo/qkKSMin/eJCT/f6RGZKUzvpHvdTV22JKjh21Rdo0NNZ417rIFsVzcsA/oiZfth2fZNx/v1SK/uuQJENCjSRpcNiwz0/a4nIShu0sO9sfTdJV/ref+uoGU+/qav/z6vjxY6beQyP+8USSFDdsZ7LKGNtU8j9vy84WZ1Q0nIgzsv73Qb5dzQPoqquuknPv3j4Wi+n+++/X/fffb20NADiHBH8VHADg3MQAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABGGO4jlbEnGnRMIvUSib9s8PSydsM3ek7L+LymX/TDpJKr1HpNHvi2zRVKqu9q9tm+ufYyVJPUdsuVqJuH8uXSxmy4IbyvvnUyXTtu3MZP3rU7LlzNWmjDc959+/ULBl9SUS/llw6VTG1LtuRpN3bW2N4aSVVCr7nys5w3kiSaWYLawvVuW/9txI3tQ7bziezc2zTL0LxSHv2li1/7H3vR3zCAgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEMTkjeJJxZVI+c1HZ4gpGRmxxZQUDMkw5Shl6p0v+kf3ZLNVpt6S/3bW1Np+D2ls9I/WkaSE/KNeisVhU+/qujrv2kxVral3pewfxVOdtGUlpWORqb5cNMS3RLZYoKQhnipRbYszmtHkH8WTTtvOq4H+fu/aKGY7PsWK7fjE4/7n+IgxKumNIz3etVHMdj+RqPK/zyoU/M+TYtEvyohHQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgJm0WXDKVVjLlma+UzHj3jWVseVPZmP8uOjkwZOpdLPsHzZUqtvyoSsq/PlVvaq3ZLbZ9WB70z9VKJmx5ejOaZ/kXG84TSYpK/ll9KVcy9S4PD5rqnfPPMkskbDfrZMp/n9c2NJh6F4r+52GxbNuHiZT/dlpqJalYMoRASooZyhvrbPvwN2+84V17+OgRU+8Zzf5rKRpiAIueh5JHQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAICZtFE88mVE86RfFU1CVd99C3DZzXWnEu7ausdbUu+Kcd20hP2DqXVc9w7s2k/JfhyQ1zvCP1pGkgYL/Pk+na0y9K5H/2islW9TLyePHvWvLI7ZonSrjr34lw7ni4p4RVv8rmfG//VRVV5t69xr24UjBP/pIkmbPmu1dm6ky5MhIKhVzpvqE4SZUbdjfkuTk3/zEoP/+lqRsvf9ayiX/dfjW8ggIABAEAwgAEIR5AL300ku65ppr1NbWplgspieffHLM9TfddJNisdiYy6pVq8ZrvQCAacI8gHK5nBYvXqwNGza8a82qVat05MiR0ctjjz12RosEAEw/5hchrF69WqtXr37Pmkwmo9bW1tNeFABg+puQ54C2bdum2bNn68ILL9Ttt9+u4+/xSphCoaCBgYExFwDA9DfuA2jVqlX63ve+p61bt+of/uEf1N3drdWrV6tSOfVHBnZ1damhoWH00t7ePt5LAgBMQuP+PqAbb7xx9N+XXnqpFi1apPPPP1/btm3TsmXL3lG/fv16rVu3bvTrgYEBhhAAnAMm/GXYCxcuVHNzs/bt23fK6zOZjOrr68dcAADT34QPoNdff13Hjx/XnDlzJvpHAQCmEPOf4IaGhsY8mjlw4IB2796tpqYmNTU16b777tOaNWvU2tqq/fv364tf/KIuuOACrVy5clwXDgCY2swDaOfOnfrUpz41+vXbz9+sXbtWGzdu1J49e/TP//zP6uvrU1tbm1asWKG//du/VSaTsf2gdK2U8VveUME/zyhfPvWLId5Nxfk/SGyIx0y9s7G8d23KmGWVqKrzrs0VbdluytqOZSE+7F07VPTP3pOk4eP++VRVxnUPD/V71+YGT5p619baMtVKFUOOXcV2PBOGjLziiP85K0ky3H76+mx5evmy/7HPR7Z9UjDm0kn++7BgvA8aLPvv88FC0dS7YcRye7NlDPowD6CrrrpK7j2CEZ977rkzWhAA4NxAFhwAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIIhx/zyg8XIiH1c68puPI5F/Flw5suUwOfn3VtGQ1yVppOSf8VSVtX1MRX/JPzuuGNkynsoF/wwuSTpZ9M/4KpZypt5J+Wd21Ue2/LXhYf8Mu1K5bOpdNubvjYz4Z3xVjFljRUOmWilh+8TiUtz/LuaNflvvkz293rVlW0yjZtXVmupzhv7H+vpMvfNl/3OlErPdpedL/r37Cv73E753hTwCAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEMWmjeN44mlcq5Rn9kM54900m/GslqVIxxGBkbL3jqWbv2hFnyxIxpMiorLSpdywyRvEYonsG87Y4lvoq/6ik4cKIqXch578TUwlbnFHBGH80EvnfVHMl/9geSaqU/PdLsnzC1NuS2tRjOWklxWr846YyNTWm3kf7bOfhqwff8K4t5P3joySpVPLfiZEt4UmR4TFIQf5RVkX5RVPxCAgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQxKTNgiuMVFQp+dWO9PnnU1VX15nWYUk9qxRs87ySMuQw5fOm3om0f05WFLflzLmoYqpX0j9r7uRQn6l1XbX/dh45etTUu2LIVGtsbDD1PjZgyxrrHxryr83510pS0feGJimdsuUdJtP+xz5TV2vqXT2zybs2kbTd1b05mDPV/+q3r3vX1mf9M9UkqZQ3ZPulbNuZSaa8a50hj9B5ZtLxCAgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEMSkjeL5+EcXqyrjF+MxPOwfJVLxjIh4m4v5x9TEjJE2aUMMRmF4xNQ7kfDvXZFxpyQTpvJ4+1zv2pF5fabehRP+9YPG+Js3R/zjWKoM54kkKesfUSNJWUOUTI0hokaSEnH/30Oz2SpT75p6/+irsvW2mTCch84SqiVlyra1/Hf/fu/aSmRbS0NdjXdtvmRbeNpw24+X/e9n4xW/dfAICAAQhGkAdXV16bLLLlNdXZ1mz56t6667Tnv37h1Tk8/n1dnZqZkzZ6q2tlZr1qxRb2/vuC4aADD1mQZQd3e3Ojs7tWPHDj3//PMqlUpasWKFcrnf/ani7rvv1tNPP60nnnhC3d3dOnz4sK6//vpxXzgAYGozPQf07LPPjvl606ZNmj17tnbt2qUrr7xS/f39euSRR7R582ZdffXVkqRHH31UH/7wh7Vjxw59/OMfH7+VAwCmtDN6Dqi/v1+S1NT01pOeu3btUqlU0vLly0drLrroIs2bN0/bt28/ZY9CoaCBgYExFwDA9HfaAyiKIt1111264oordMkll0iSenp6lE6n1djYOKa2paVFPT09p+zT1dWlhoaG0Ut7e/vpLgkAMIWc9gDq7OzUq6++qscff/yMFrB+/Xr19/ePXg4dOnRG/QAAU8NpvQ/ojjvu0DPPPKOXXnpJc+f+7j0era2tKhaL6uvrG/MoqLe3V62trafslclklMnYPuYXADD1mR4BOed0xx13aMuWLXrxxRe1YMGCMdcvWbJEqVRKW7duHf3e3r17dfDgQXV0dIzPigEA04LpEVBnZ6c2b96sp556SnV1daPP6zQ0NCibzaqhoUE333yz1q1bp6amJtXX1+vOO+9UR0cHr4ADAIxhGkAbN26UJF111VVjvv/oo4/qpptukiR985vfVDwe15o1a1QoFLRy5Up997vfHZfFAgCmD9MAch55SlVVVdqwYYM2bNhw2ouSpJZZGWWr/PKyCmX/zSgZA6cs1bG4LSMtachriytr6m3J96pEFVPvsmxZVpbtTMxrMPX++X+c8K6NG/LUJKlQ9t8vDa0zTL0/9JHzTfUJwy7Ppm3nSlL+OXbOGHkXS/vv83LJeB4agh0rhmMpScWK7dW4g0P93rVHDh419W5b4J+lODjQZ+odFfP+xXFDfmHM79iQBQcACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACOK0Po7hbIiSQ4pSftEPLu4fs+GStkgOSxZPLGab584Q3RNL+cfZSJJHatLvaiu2feIMESiSFCX8tzNdVWXqHc/6r71oiR2RFEX+OzFbZ7spnTev1lSfcP77vDpljG1y/udtqVw29bbENsVjtn0YGU7ysjGCK5ayfUTMB86f5V37378+YOrdn/M/b+OWG76kUqnkXZt3/rUl+d0ueQQEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACIIBBAAIggEEAAiCAQQACGLSZsFVpZOqSvstL+n8s8bKkS0TSoYsK2fMYbL0jhvy7iQpLkPOXMKWMxeLxWz1cf/6TMp2SjY3N3jXvlY5aOrdYMila6i2ZYdlUrbz0EX+GWxlN2LrHfff5/G07dinDDeJZNLW25LVl0jYbptRzJYb2N7unwUXz/jfNiXpYG+vd+2celvGYI3nfawklSuG7D3PWh4BAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCmLRRPOlYRplY2qs2aYjAiWK2CJR4wj8exBmidSQpMsQCWeNvLFE82VTW1DuZskX3WCKKUsYontbZzd611cbe9TObvGtn19WZeqdkO56VhP/xrBgiUyQpcv7nYSJui5FJGtadNB4fV/Zfdzxm+127HBVN9ee1+p8rjTNscTknTwx619Y1+kdTSVKp4B/xFTdEnsU9T0EeAQEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCmLRZcFE5UuSZ9VQplrz7OkP+miTF0/65Z4mUMScr5l8fRf6ZTZJUqZS9a8sx//33FlvWmI3t+GSq/I9PY6Mtry1tyL5KGY/PSH7EVK+M4aYa2XLmLMezXLEdH2dYdsKY1+bKln1u2ycJY/birMYa79p5rTNNvTNJ//NwXvtsU+9f/Xyfodpy+/HbfzwCAgAEYRpAXV1duuyyy1RXV6fZs2fruuuu0969e8fUXHXVVYrFYmMut91227guGgAw9ZkGUHd3tzo7O7Vjxw49//zzKpVKWrFihXK53Ji6W265RUeOHBm9PPjgg+O6aADA1Gd6DujZZ58d8/WmTZs0e/Zs7dq1S1deeeXo96urq9Xa2jo+KwQATEtn9BxQf3+/JKmpaeyHMX3/+99Xc3OzLrnkEq1fv17Dw8Pv2qNQKGhgYGDMBQAw/Z32q+CiKNJdd92lK664Qpdccsno9z/72c9q/vz5amtr0549e/SlL31Je/fu1Y9+9KNT9unq6tJ99913ussAAExRpz2AOjs79eqrr+qnP/3pmO/feuuto/++9NJLNWfOHC1btkz79+/X+eef/44+69ev17p160a/HhgYUHt7++kuCwAwRZzWALrjjjv0zDPP6KWXXtLcuXPfs3bp0qWSpH379p1yAGUyGWUymdNZBgBgCjMNIOec7rzzTm3ZskXbtm3TggUL3vf/7N69W5I0Z86c01ogAGB6Mg2gzs5Obd68WU899ZTq6urU09MjSWpoaFA2m9X+/fu1efNm/cmf/IlmzpypPXv26O6779aVV16pRYsWTcgGAACmJtMA2rhxo6S33mz6fz366KO66aablE6n9cILL+ihhx5SLpdTe3u71qxZo6985SvjtmAAwPRg/hPce2lvb1d3d/cZLWj0ZymS88wFM+WkGbPgyv6RaqbMJklKGXLmKpHtFfMVV/SuHSnnTb1jttgzxQw5XNZYuor8D9DMmY2m3kMn/N8SkOvvM/VO5GxPv6bj1d61ych2HiYM78aIJ20ZaZXI//hExnPcGW6cxgg7RTHDDV9SNuO/zy+5wPYiq8baKu/aWU3+tZL0a/nfTySS/scn8swXJAsOABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABAEAwgAEAQDCAAQBAMIABDEaX8e0IRz7n2jf95Wcf7ZMJZYGEmSIeanWLHFd7iK/+4vlP0jMyQpZkhjKVryhiRVKrYsnpghByVtiCeSpGQ67V9r/NSPVJ3/8SmWCqbelZItc6hsqE/Fbb9XpmP+twlLfJQklQxRPPG47bYZz/gf+0rReI4bbxOu4n98Fl14gal3c33WUG1b9wUL53vXzpxZ711bLPmtg0dAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAmbRZcJYpUifwyxCJDvlsyZdvksmcenSSNFP0zzySpMDzoXdt78qipd31drXdtpeS/jZKUN+aeZRL++7ypvsHUOyVDVl9xyNS7tqnauzYbtx37xqx/rpYkjRhqjZGEiqX8893KxnNlMO9/rpwoDpt6p1JV3rWWTDpJipztHC9F/r/LLzj/w6beTTP8b8uvH/6tqfeqlYu9a0eKJ/1r80VJ2963jkdAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgGEAAgCAYQACAIBhAAIAgJm0Uj2Lxty4ekoYokcjZIlPe7M951x4dsASmSCcH/KN4jvefMPWuqc5615YKtpiSobytPpHKeNfWZ/3jPiQpm/SP4mms+Ec2SVLZECOjtO2mlDtp24dvlPP+vYds52HWcHzyFVsUz7GBAe/aoSFbFE/M8PtzJWZbdzprO1dmVjd6186d94em3qrxP/apGtvt59KPfcy7tq//kHdtbthvzTwCAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAAQxabPgkpmUUhm/jLdCoejdt1z0zw6TpKGcf15bOuGfSSdJH2hr865d+IG5pt7O+WeN5QZt2WFHT/jvE0na+z9veNf+9wn/vClJpjP4qlnzTa3Lg/45gLHmJlPvQ6/1mup3HPwf79piZMs9s8QjViq2209k6J1I2H4fTiT8D34xsmXvlZ3/fYokzcg2etf+v9VVpt4/27vfu3bo5Jum3pf/Ydq7NmHIO0yUE151PAICAARhGkAbN27UokWLVF9fr/r6enV0dOjHP/7x6PX5fF6dnZ2aOXOmamtrtWbNGvX22n7TAwCcG0wDaO7cuXrggQe0a9cu7dy5U1dffbWuvfZa/eIXv5Ak3X333Xr66af1xBNPqLu7W4cPH9b1118/IQsHAExtpueArrnmmjFf//3f/702btyoHTt2aO7cuXrkkUe0efNmXX311ZKkRx99VB/+8Ie1Y8cOffzjHx+/VQMAprzTfg6oUqno8ccfVy6XU0dHh3bt2qVSqaTly5eP1lx00UWaN2+etm/f/q59CoWCBgYGxlwAANOfeQD9/Oc/V21trTKZjG677TZt2bJFF198sXp6epROp9XY2DimvqWlRT09Pe/ar6urSw0NDaOX9vZ280YAAKYe8wC68MILtXv3br388su6/fbbtXbtWv3yl7887QWsX79e/f39o5dDh4wvwwUATEnm9wGl02ldcMEFkqQlS5bov/7rv/Stb31LN9xwg4rFovr6+sY8Curt7VVra+u79stkMspk/D+THgAwPZzx+4CiKFKhUNCSJUuUSqW0devW0ev27t2rgwcPqqOj40x/DABgmjE9Alq/fr1Wr16tefPmaXBwUJs3b9a2bdv03HPPqaGhQTfffLPWrVunpqYm1dfX684771RHRwevgAMAvINpAB09elR/9md/piNHjqihoUGLFi3Sc889pz/+4z+WJH3zm99UPB7XmjVrVCgUtHLlSn33u989rYVVoorKkV/sR9mQ9xEzxn20Njd4185onGnqXVNd711bLOdNvUtlQ7xOqy1GJopskUMtDbXetf/1i9dMvd845h8LVFfnv78lqVyb9a4dyJdMvasa/feJJM2c2ehdW3ExU29LFE/Skq0jqcoQ31JXZ9snNTX+x+ek8dW1fYO2uKm6av/bflWVLYpn737/28RvDhw09V6xos+7Nmu42Vc8T0HTAHrkkUfe8/qqqipt2LBBGzZssLQFAJyDyIIDAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEwQACAATBAAIABMEAAgAEYU7DnmjOOUnSyEjR+/8UCv4xKM6SOyKpUPTvbVmzJMVU8K4tVvxrJalctqzF9ntIZIxjsRyfcskvfultlbL/Wgqlsq23868vG3s7w3n1Vn///WKO4nGGYuOxL8f811Iy7sNi0b/e2tt6Hlr6D+eGJ6x3pWI7PsPD/pFdUcq/dnj4regw9z4nV8y9X8VZ9vrrr/OhdAAwDRw6dEhz58591+sn3QCKokiHDx9WXV2dYv/nt6eBgQG1t7fr0KFDqq+3hUpOJWzn9HEubKPEdk4347GdzjkNDg6qra1N8fi7/4Vl0v0JLh6Pv+fErK+vn9YH/21s5/RxLmyjxHZON2e6nQ0N7/9JArwIAQAQBAMIABDElBlAmUxG9957rzKZTOilTCi2c/o4F7ZRYjunm7O5nZPuRQgAgHPDlHkEBACYXhhAAIAgGEAAgCAYQACAIKbMANqwYYM+8IEPqKqqSkuXLtV//ud/hl7SuPra176mWCw25nLRRReFXtYZeemll3TNNdeora1NsVhMTz755JjrnXO65557NGfOHGWzWS1fvlyvvfZamMWegffbzptuuukdx3bVqlVhFnuaurq6dNlll6murk6zZ8/Wddddp717946pyefz6uzs1MyZM1VbW6s1a9aot7c30IpPj892XnXVVe84nrfddlugFZ+ejRs3atGiRaNvNu3o6NCPf/zj0evP1rGcEgPoBz/4gdatW6d7771XP/vZz7R48WKtXLlSR48eDb20cfWRj3xER44cGb389Kc/Db2kM5LL5bR48WJt2LDhlNc/+OCD+va3v62HH35YL7/8smpqarRy5Url8/mzvNIz837bKUmrVq0ac2wfe+yxs7jCM9fd3a3Ozk7t2LFDzz//vEqlklasWKFcLjdac/fdd+vpp5/WE088oe7ubh0+fFjXX399wFXb+WynJN1yyy1jjueDDz4YaMWnZ+7cuXrggQe0a9cu7dy5U1dffbWuvfZa/eIXv5B0Fo+lmwIuv/xy19nZOfp1pVJxbW1trqurK+Cqxte9997rFi9eHHoZE0aS27Jly+jXURS51tZW9/Wvf330e319fS6TybjHHnsswArHx+9vp3POrV271l177bVB1jNRjh496iS57u5u59xbxy6VSrknnnhitOZXv/qVk+S2b98eapln7Pe30znn/uiP/sj95V/+ZbhFTZAZM2a4f/zHfzyrx3LSPwIqFovatWuXli9fPvq9eDyu5cuXa/v27QFXNv5ee+01tbW1aeHChfrc5z6ngwcPhl7ShDlw4IB6enrGHNeGhgYtXbp02h1XSdq2bZtmz56tCy+8ULfffruOHz8eeklnpL+/X5LU1NQkSdq1a5dKpdKY43nRRRdp3rx5U/p4/v52vu373/++mpubdckll2j9+vUaHrZ9xMJkUqlU9PjjjyuXy6mjo+OsHstJF0b6+44dO6ZKpaKWlpYx329padGvf/3rQKsaf0uXLtWmTZt04YUX6siRI7rvvvv0yU9+Uq+++qrq6upCL2/c9fT0SNIpj+vb100Xq1at0vXXX68FCxZo//79+pu/+RutXr1a27dvVyKRCL08syiKdNddd+mKK67QJZdcIumt45lOp9XY2Dimdiofz1NtpyR99rOf1fz589XW1qY9e/boS1/6kvbu3asf/ehHAVdr9/Of/1wdHR3K5/Oqra3Vli1bdPHFF2v37t1n7VhO+gF0rli9evXovxctWqSlS5dq/vz5+uEPf6ibb7454Mpwpm688cbRf1966aVatGiRzj//fG3btk3Lli0LuLLT09nZqVdffXXKP0f5ft5tO2+99dbRf1966aWaM2eOli1bpv379+v8888/28s8bRdeeKF2796t/v5+/cu//IvWrl2r7u7us7qGSf8nuObmZiUSiXe8AqO3t1etra2BVjXxGhsb9aEPfUj79u0LvZQJ8faxO9eOqyQtXLhQzc3NU/LY3nHHHXrmmWf0k5/8ZMzHprS2tqpYLKqvr29M/VQ9nu+2naeydOlSSZpyxzOdTuuCCy7QkiVL1NXVpcWLF+tb3/rWWT2Wk34ApdNpLVmyRFu3bh39XhRF2rp1qzo6OgKubGINDQ1p//79mjNnTuilTIgFCxaotbV1zHEdGBjQyy+/PK2Pq/TWp/4eP358Sh1b55zuuOMObdmyRS+++KIWLFgw5volS5YolUqNOZ579+7VwYMHp9TxfL/tPJXdu3dL0pQ6nqcSRZEKhcLZPZbj+pKGCfL444+7TCbjNm3a5H75y1+6W2+91TU2Nrqenp7QSxs3f/VXf+W2bdvmDhw44P793//dLV++3DU3N7ujR4+GXtppGxwcdK+88op75ZVXnCT3jW98w73yyivut7/9rXPOuQceeMA1Nja6p556yu3Zs8dde+21bsGCBW5kZCTwym3eazsHBwfdF77wBbd9+3Z34MAB98ILL7iPfexj7oMf/KDL5/Ohl+7t9ttvdw0NDW7btm3uyJEjo5fh4eHRmttuu83NmzfPvfjii27nzp2uo6PDdXR0BFy13ftt5759+9z999/vdu7c6Q4cOOCeeuopt3DhQnfllVcGXrnNl7/8Zdfd3e0OHDjg9uzZ47785S+7WCzm/u3f/s05d/aO5ZQYQM45953vfMfNmzfPpdNpd/nll7sdO3aEXtK4uuGGG9ycOXNcOp125513nrvhhhvcvn37Qi/rjPzkJz9xkt5xWbt2rXPurZdif/WrX3UtLS0uk8m4ZcuWub1794Zd9Gl4r+0cHh52K1ascLNmzXKpVMrNnz/f3XLLLVPul6dTbZ8k9+ijj47WjIyMuL/4i79wM2bMcNXV1e7Tn/60O3LkSLhFn4b3286DBw+6K6+80jU1NblMJuMuuOAC99d//deuv78/7MKN/vzP/9zNnz/fpdNpN2vWLLds2bLR4ePc2TuWfBwDACCISf8cEABgemIAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIJgAAEAgmAAAQCCYAABAIL4/+w9RqeUplcmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch, (X, Y) = next(enumerate(train_dataloader))\n",
    "plt.imshow(X[0].cpu().permute((1,2,0))); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below is important as our models get bigger: this is wrapping the pytorch data loaders to put the data onto the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "    # CIFAR-10 is *color* images so 3 layers!\n",
    "    return x.view(-1, 3, 32, 32).to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "\n",
    "train_dataloader = WrappedDataLoader(train_dataloader, preprocess)\n",
    "val_dataloader = WrappedDataLoader(val_dataloader, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Downsampler(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, shape, stride=2):\n",
    "        super(Downsampler, self).__init__()\n",
    "\n",
    "        self.norm = nn.LayerNorm([in_channels, *shape])\n",
    "\n",
    "        self.downsample = nn.Conv2d(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels,\n",
    "            kernel_size = stride,\n",
    "            stride = stride,\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "\n",
    "\n",
    "        return self.downsample(self.norm(inputs))\n",
    "        \n",
    "        \n",
    "\n",
    "class ConvNextBlock(nn.Module):\n",
    "    \"\"\"This block of operations is loosely based on this paper:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, shape):\n",
    "        super(ConvNextBlock, self).__init__()\n",
    "\n",
    "        # Depthwise, seperable convolution with a large number of output filters:\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                                     out_channels=in_channels, \n",
    "                                     groups=in_channels,\n",
    "                                     kernel_size=[7,7],\n",
    "                                     padding='same' )\n",
    "\n",
    "        self.norm = nn.LayerNorm([in_channels, *shape])\n",
    "\n",
    "        # Two more convolutions:\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_channels, \n",
    "                                     out_channels=4*in_channels,\n",
    "                                     kernel_size=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=4*in_channels, \n",
    "                                     out_channels=in_channels,\n",
    "                                     kernel_size=1\n",
    "                                     )\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "\n",
    "        # The normalization layer:\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # The non-linear activation layer:\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # This makes it a residual network:\n",
    "        return x + inputs\n",
    "    \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, n_initial_filters, n_stages, blocks_per_stage):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # This is a downsampling convolution that will produce patches of output.\n",
    "\n",
    "        # This is similar to what vision transformers do to tokenize the images.\n",
    "        self.stem = nn.Conv2d(in_channels=3,\n",
    "                                    out_channels=n_initial_filters,\n",
    "                                    kernel_size=1,\n",
    "                                    stride=1)\n",
    "        \n",
    "        current_shape = [32, 32]\n",
    "\n",
    "        self.norm1 = nn.LayerNorm([n_initial_filters,*current_shape])\n",
    "        # self.norm1 = WrappedLayerNorm()\n",
    "\n",
    "        current_n_filters = n_initial_filters\n",
    "        \n",
    "        self.layers = nn.Sequential()\n",
    "        for i, n_blocks in enumerate(range(n_stages)):\n",
    "            # Add a convnext block series:\n",
    "            for _ in range(blocks_per_stage):\n",
    "                self.layers.append(ConvNextBlock(in_channels=current_n_filters, shape=current_shape))\n",
    "            # Add a downsampling layer:\n",
    "            if i != n_stages - 1:\n",
    "                # Skip downsampling if it's the last layer!\n",
    "                self.layers.append(Downsampler(\n",
    "                    in_channels=current_n_filters, \n",
    "                    out_channels=2*current_n_filters,\n",
    "                    shape = current_shape,\n",
    "                    )\n",
    "                )\n",
    "                # Double the number of filters:\n",
    "                current_n_filters = 2*current_n_filters\n",
    "                # Cut the shape in half:\n",
    "                current_shape = [ cs // 2 for cs in current_shape]\n",
    "            \n",
    "\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.LayerNorm(current_n_filters),\n",
    "            nn.Linear(current_n_filters, 10)\n",
    "        )\n",
    "        # self.norm2 = nn.InstanceNorm2d(current_n_filters)\n",
    "        # # This brings it down to one channel / class\n",
    "        # self.bottleneck = nn.Conv2d(in_channels=current_n_filters, out_channels=10, \n",
    "        #                                   kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = self.stem(inputs)\n",
    "        # Apply a normalization after the initial patching:\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Apply the main chunk of the network:\n",
    "        x = self.layers(x)\n",
    "\n",
    "        # Normalize and readout:\n",
    "        x = nn.functional.avg_pool2d(x, x.shape[2:])\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "        # x = self.norm2(x)\n",
    "        # x = self.bottleneck(x)\n",
    "\n",
    "        # # Average pooling of the remaining spatial dimensions (and reshape) makes this label-like:\n",
    "        # return nn.functional.avg_pool2d(x, kernel_size=x.shape[-2:]).reshape((-1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo # if not on Polaris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Classifier                               [128, 10]                 --\n",
      "├─Conv2d: 1-1                            [128, 32, 32, 32]         128\n",
      "├─LayerNorm: 1-2                         [128, 32, 32, 32]         65,536\n",
      "├─Sequential: 1-3                        [128, 256, 4, 4]          --\n",
      "│    └─ConvNextBlock: 2-1                [128, 32, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-1                  [128, 32, 32, 32]         1,600\n",
      "│    │    └─LayerNorm: 3-2               [128, 32, 32, 32]         65,536\n",
      "│    │    └─Conv2d: 3-3                  [128, 128, 32, 32]        4,224\n",
      "│    │    └─Conv2d: 3-4                  [128, 32, 32, 32]         4,128\n",
      "│    └─ConvNextBlock: 2-2                [128, 32, 32, 32]         --\n",
      "│    │    └─Conv2d: 3-5                  [128, 32, 32, 32]         1,600\n",
      "│    │    └─LayerNorm: 3-6               [128, 32, 32, 32]         65,536\n",
      "│    │    └─Conv2d: 3-7                  [128, 128, 32, 32]        4,224\n",
      "│    │    └─Conv2d: 3-8                  [128, 32, 32, 32]         4,128\n",
      "│    └─Downsampler: 2-3                  [128, 64, 16, 16]         --\n",
      "│    │    └─LayerNorm: 3-9               [128, 32, 32, 32]         65,536\n",
      "│    │    └─Conv2d: 3-10                 [128, 64, 16, 16]         8,256\n",
      "│    └─ConvNextBlock: 2-4                [128, 64, 16, 16]         --\n",
      "│    │    └─Conv2d: 3-11                 [128, 64, 16, 16]         3,200\n",
      "│    │    └─LayerNorm: 3-12              [128, 64, 16, 16]         32,768\n",
      "│    │    └─Conv2d: 3-13                 [128, 256, 16, 16]        16,640\n",
      "│    │    └─Conv2d: 3-14                 [128, 64, 16, 16]         16,448\n",
      "│    └─ConvNextBlock: 2-5                [128, 64, 16, 16]         --\n",
      "│    │    └─Conv2d: 3-15                 [128, 64, 16, 16]         3,200\n",
      "│    │    └─LayerNorm: 3-16              [128, 64, 16, 16]         32,768\n",
      "│    │    └─Conv2d: 3-17                 [128, 256, 16, 16]        16,640\n",
      "│    │    └─Conv2d: 3-18                 [128, 64, 16, 16]         16,448\n",
      "│    └─Downsampler: 2-6                  [128, 128, 8, 8]          --\n",
      "│    │    └─LayerNorm: 3-19              [128, 64, 16, 16]         32,768\n",
      "│    │    └─Conv2d: 3-20                 [128, 128, 8, 8]          32,896\n",
      "│    └─ConvNextBlock: 2-7                [128, 128, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-21                 [128, 128, 8, 8]          6,400\n",
      "│    │    └─LayerNorm: 3-22              [128, 128, 8, 8]          16,384\n",
      "│    │    └─Conv2d: 3-23                 [128, 512, 8, 8]          66,048\n",
      "│    │    └─Conv2d: 3-24                 [128, 128, 8, 8]          65,664\n",
      "│    └─ConvNextBlock: 2-8                [128, 128, 8, 8]          --\n",
      "│    │    └─Conv2d: 3-25                 [128, 128, 8, 8]          6,400\n",
      "│    │    └─LayerNorm: 3-26              [128, 128, 8, 8]          16,384\n",
      "│    │    └─Conv2d: 3-27                 [128, 512, 8, 8]          66,048\n",
      "│    │    └─Conv2d: 3-28                 [128, 128, 8, 8]          65,664\n",
      "│    └─Downsampler: 2-9                  [128, 256, 4, 4]          --\n",
      "│    │    └─LayerNorm: 3-29              [128, 128, 8, 8]          16,384\n",
      "│    │    └─Conv2d: 3-30                 [128, 256, 4, 4]          131,328\n",
      "│    └─ConvNextBlock: 2-10               [128, 256, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-31                 [128, 256, 4, 4]          12,800\n",
      "│    │    └─LayerNorm: 3-32              [128, 256, 4, 4]          8,192\n",
      "│    │    └─Conv2d: 3-33                 [128, 1024, 4, 4]         263,168\n",
      "│    │    └─Conv2d: 3-34                 [128, 256, 4, 4]          262,400\n",
      "│    └─ConvNextBlock: 2-11               [128, 256, 4, 4]          --\n",
      "│    │    └─Conv2d: 3-35                 [128, 256, 4, 4]          12,800\n",
      "│    │    └─LayerNorm: 3-36              [128, 256, 4, 4]          8,192\n",
      "│    │    └─Conv2d: 3-37                 [128, 1024, 4, 4]         263,168\n",
      "│    │    └─Conv2d: 3-38                 [128, 256, 4, 4]          262,400\n",
      "├─Sequential: 1-4                        [128, 10]                 --\n",
      "│    └─Flatten: 2-12                     [128, 256]                --\n",
      "│    └─LayerNorm: 2-13                   [128, 256]                512\n",
      "│    └─Linear: 2-14                      [128, 10]                 2,570\n",
      "==========================================================================================\n",
      "Total params: 2,047,114\n",
      "Trainable params: 2,047,114\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 10.34\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 1036.27\n",
      "Params size (MB): 8.19\n",
      "Estimated Total Size (MB): 1046.03\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = Classifier(32, 4, 2).to(device=dev)\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model, input_size=(batch_size, 3, 32, 32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, loss_fn, val_bar):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            val_bar.update()\n",
    "            \n",
    "    loss /= num_batches\n",
    "    correct /= (size*batch_size)\n",
    "    \n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader, model, loss_fn, optimizer, progress_bar):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()      \n",
    "\n",
    "        progress_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef94e5427fe4d498339725d4d039f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 0:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c8c030d5194382b6227be437618fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate (train) Epoch 0:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss: 2.242, accuracy: 17.150\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5561dcb32b4aa29f5146241b49c0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 0:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: validation loss: 2.245, accuracy: 17.138\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1044b744a5e48958a1d78d7ee4f3dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 1:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476262862c81474fb331705253ced8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 1:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: validation loss: 2.049, accuracy: 23.586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc386b8d027748fb84bf20bb694a4eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 2:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dcb800e1f6d484eb9e59c97376a6a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 2:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: validation loss: 1.961, accuracy: 28.194\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfd2e0fac494549adb76d9783af247c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 3:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71e01f3496c47e1882939282c6ceb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 3:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: validation loss: 1.931, accuracy: 28.649\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb50ac58359148d0ae98551722cabeaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 4:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a022ee95c40c42be9f892d05551075cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 4:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: validation loss: 1.898, accuracy: 30.973\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6201f8291291491c80f5f6d85ae1ea56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 5:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edf63c8b2974923bea09401398abe14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate (train) Epoch 5:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: training loss: 1.882, accuracy: 32.066\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720b6b0c55b244e88b1789928f066f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 5:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: validation loss: 1.889, accuracy: 32.051\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116747ef7dda4256bd96e2aaba9d6b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 6:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f920e609ec0a444e91151e863b48272e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 6:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: validation loss: 1.802, accuracy: 34.800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42b66d3dfb14b29a6fafd0af5c1bf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 7:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3640bc4759414a35961e1e5935f1bcc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 7:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: validation loss: 1.770, accuracy: 35.127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f26fd374dd4057baa544efb2a2e5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 8:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab9c877d07941e4b8ea29fe53378f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 8:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: validation loss: 1.726, accuracy: 36.719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b69d23eb7c4cc9b529a9962b26fbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 9:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afafaec24784a739d2867744f619187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 9:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: validation loss: 1.701, accuracy: 38.074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f59b554c7514e5bab7c1737f5546682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 10:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1f862bfd754fdb85a2944f9a872632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate (train) Epoch 10:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: training loss: 1.709, accuracy: 39.327\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a983390005444abdd45368aec23ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 10:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: validation loss: 1.711, accuracy: 38.776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5ee47bd7794f328d375369b10e6ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 11:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95fcfd921d94449da08bc2e5f7548337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 11:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: validation loss: 1.657, accuracy: 39.270\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c8474a26744b029be57c0dc8a7caf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 12:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a2eda6ca9948f9a501bd12ace0d0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 12:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: validation loss: 1.653, accuracy: 39.537\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd28c74bb7a47aba1eda46aef9288ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 13:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb5c2fa41d74cb6806a23164362bc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 13:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: validation loss: 1.678, accuracy: 39.389\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689141707fd3414982a2fb715b96cea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 14:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74aba3dd787b43c4bdf0d70a0b720c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 14:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: validation loss: 1.650, accuracy: 40.051\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a536fd4ecc4438817c7d0fdddeac47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 15:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d09588ff874b71b31ffb360b409a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate (train) Epoch 15:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: training loss: 1.616, accuracy: 42.050\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945b38e4b72c4d2a8c17ed1fccb2d6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 15:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: validation loss: 1.610, accuracy: 42.029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ac9f968ba94f61aa2541444130f7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 16:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a120e80ba914178937d63cb111e652e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 16:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: validation loss: 1.565, accuracy: 42.840\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fcfe59485a64fa08ca97e50afb27100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 17:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ee946e3fb94776977fb8045c471ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 17:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: validation loss: 1.578, accuracy: 43.206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7bc4a98f3f4e199b491e72e202927e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 18:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439bc963e9e9431b844b23a7fdfaaedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 18:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: validation loss: 1.546, accuracy: 43.681\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc2289cfd994d1780dd5565b6bf5785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 19:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ffc50969314a8d941ac209967ff426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 19:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: validation loss: 1.542, accuracy: 43.730\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6227f7ef6be145fa934e2eeffd48468a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 20:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f796de8eb29b4ada922e27a871e43c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate (train) Epoch 20:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: training loss: 1.499, accuracy: 46.069\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b00b8685d14614b0937a8d37c4733a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 20:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: validation loss: 1.495, accuracy: 45.609\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f0a5477ade41debaebde0784c3d039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 21:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebacc521b2174d5d896329cb65f5fc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 21:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: validation loss: 1.545, accuracy: 43.523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f53c9a5ddea4ffaafc9f88e363e3102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 22:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d7724c8c6746b7ae72d617e2d0197a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 22:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: validation loss: 1.488, accuracy: 46.766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baac98d6a187423fb2d24c1b2168eac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 23:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07aac2b2b97b48beb27f6ea9ade4ddc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 23:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: validation loss: 1.555, accuracy: 43.335\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59cb83e885ef487abbdff49f6e7dd2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 24:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4932a50ed542e8947b5d3da8be3ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 24:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: validation loss: 1.483, accuracy: 45.204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6abc05896746d8b315587c1083eb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 25:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abb31c41c884d18b1a1e8e0ff9f73f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate (train) Epoch 25:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: training loss: 1.471, accuracy: 46.950\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff5107bf1e7435b9596fdb2767793de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 25:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: validation loss: 1.474, accuracy: 46.677\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe00a300da74bda8ef1732f147e44c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 26:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b675f4a23cc427aa59005a74cdd75db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 26:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: validation loss: 1.507, accuracy: 44.719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862ef4b8b06946a28aa6bf4455975138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 27:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2eb91e74b08459fb74481e64ec5e86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 27:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: validation loss: 1.466, accuracy: 46.232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cdadfd8b534a4d97d1247ac53d7b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 28:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e99f31288240a781fddc94424aa5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 28:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: validation loss: 1.436, accuracy: 47.903\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5722f933dad64d9ab524363a385b6453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 29:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0d62bc3c8a44969ac825ba06b4de74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validate Epoch 29:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: validation loss: 1.433, accuracy: 48.072\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "for j in range(epochs):\n",
    "    with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Train Epoch {j}\") as train_bar:\n",
    "        train_one_epoch(train_dataloader, model, loss_fn, optimizer, train_bar)\n",
    "    \n",
    "    # checking on the training & validation loss & accuracy \n",
    "    # for training data - only once every 5 epochs (takes a while) \n",
    "    if j % 5 == 0:\n",
    "        with tqdm(total=len(train_dataloader), position=0, leave=True, desc=f\"Validate (train) Epoch {j}\") as train_eval:\n",
    "            acc, loss = evaluate(train_dataloader, model, loss_fn, train_eval)\n",
    "            print(f\"Epoch {j}: training loss: {loss:.3f}, accuracy: {acc:.3f}\")\n",
    "            \n",
    "    with tqdm(total=len(val_dataloader), position=0, leave=True, desc=f\"Validate Epoch {j}\") as val_bar:\n",
    "    \n",
    "        acc_val, loss_val = evaluate(val_dataloader, model, loss_fn, val_bar)\n",
    "        print(f\"Epoch {j}: validation loss: {loss_val:.3f}, accuracy: {acc_val:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1:\n",
    "\n",
    "In this notebook, we've learned about some basic convolutional networks and trained one on CIFAR-10 images.  It did ... OK.  There is significant overfitting of this model.  There are some ways to address that, but we didn't have time to get into that in this session.\n",
    "\n",
    "Meanwhile, your homework (part 1) for this week is to try to train the model again but with a different architecture.  Change one or more of the following:\n",
    "- The number of convolutions between downsampling\n",
    "- The number of filters in each layer\n",
    "- The initial \"patchify\" layer\n",
    "- Another hyper-parameter of your choosing\n",
    "\n",
    "\n",
    "And compare your final validation accuracy to the accuracy shown here.  Can you beat the validation accuracy shown?\n",
    "\n",
    "For full credit on the homework, you need to show (via text, or make a plot) the training and validation data sets' performance (loss and accuracy) for all the epochs you train.  You also need to explain, in several sentences, what you changed in the network and why you think it makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
